{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "name": "Module 04 \u2014 Bike Sharing NN (Executive Summary)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04 \u2014 Neural Networks: Bike Sharing Demand Prediction\n\n## Executive Summary \u2014 WelcomeBike DC Expansion\n\n**Scenario**: WelcomeBike (Beijing-based bike-sharing company) expanding to Washington DC.\n**Task**: Build a neural network to predict hourly bike rental volumes.\n**Stakeholders**: Zhao (CEO), William (Investment Banker), Johnny (Data Science Intern)\n**Framework**: scikit-learn MLPRegressor (neural network) + MinMaxScaler preprocessing\n**Target**: Predict total hourly bike rentals (casual + registered)"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.inspection import permutation_importance\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(\"Using scikit-learn MLPRegressor for neural network\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Initial Exploration"
   ],
   "id": "cell-2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data URLs\nBIKES_URL = \"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv\"\nHOLDOUT_URL = \"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes_december.csv\"\nMINI_URL = \"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/biking_holdout_test_mini.csv\"\nMINI_ANSWERS_URL = \"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/biking_holdout_test_mini_answers.csv\"\n\n# Load training data\ndf = pd.read_csv(BIKES_URL)\n\nprint(f\"Training data shape: {df.shape}\")\nprint(f\"Columns: {list(df.columns)}\")\nprint(f\"\\nData types:\\n{df.dtypes}\")\ndf.head()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Basic statistics and missing values\nprint(\"=== Descriptive Statistics ===\")\ndisplay(df.describe().round(2))\nprint(f\"\\n=== Missing Values ===\")\nprint(df.isnull().sum())\nprint(f\"\\nTotal missing: {df.isnull().sum().sum()}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ],
   "id": "cell-5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create target variable\ndf['total'] = df['casual'] + df['registered']\nprint(f\"Target (total hourly rentals) statistics:\")\nprint(f\"  Mean:   {df['total'].mean():.1f}\")\nprint(f\"  Median: {df['total'].median():.1f}\")\nprint(f\"  Std:    {df['total'].std():.1f}\")\nprint(f\"  Min:    {df['total'].min()}, Max: {df['total'].max()}\")\nprint(f\"  Skew:   {df['total'].skew():.2f}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-6"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Rental patterns by hour, season, weather, and temperature\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Hourly pattern\nhourly = df.groupby('hr')['total'].mean()\naxes[0,0].bar(hourly.index, hourly.values, color='steelblue')\naxes[0,0].set_xlabel('Hour of Day')\naxes[0,0].set_ylabel('Average Total Rentals')\naxes[0,0].set_title('Average Bike Rentals by Hour')\n\n# Seasonal pattern\nseasonal = df.groupby('season')['total'].mean()\nseason_labels = {1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Fall'}\naxes[0,1].bar([season_labels[s] for s in seasonal.index], seasonal.values, color='coral')\naxes[0,1].set_ylabel('Average Total Rentals')\naxes[0,1].set_title('Average Bike Rentals by Season')\n\n# Weather pattern\nweather = df.groupby('weathersit')['total'].mean()\nweather_labels = {1: 'Clear', 2: 'Mist', 3: 'Light Rain', 4: 'Heavy Rain'}\nw_labels = [weather_labels.get(w, str(w)) for w in weather.index]\naxes[1,0].bar(w_labels, weather.values, color='mediumseagreen')\naxes[1,0].set_ylabel('Average Total Rentals')\naxes[1,0].set_title('Average Bike Rentals by Weather')\n\n# Temperature vs rentals\naxes[1,1].scatter(df['temp_c'], df['total'], alpha=0.05, s=5, color='steelblue')\naxes[1,1].set_xlabel('Temperature (Celsius)')\naxes[1,1].set_ylabel('Total Rentals')\naxes[1,1].set_title('Temperature vs Bike Rentals')\n\nplt.tight_layout()\nplt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-7"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Correlation heatmap\nnumeric_cols = ['hr', 'temp_c', 'feels_like_c', 'hum', 'windspeed', 'total']\ncorr = df[numeric_cols].corr()\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr, annot=True, cmap='coolwarm', center=0, fmt='.2f')\nplt.title('Feature Correlations')\nplt.tight_layout()\nplt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n\nBased on the data exploration, we engineer features to capture:\n- **Temporal patterns**: Hour of day (cyclical + one-hot), day of week, month\n- **Weather interactions**: Temperature x humidity, temperature x windspeed\n- **Behavioral patterns**: Rush hour, nighttime, weekend indicators\n- **Non-linear effects**: Temperature squared\n\nThe one-hot encoding of hour is critical because rental patterns are highly non-linear across hours (dual peaks at rush hours)."
   ],
   "id": "cell-9"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def engineer_features(data, is_training=True):\n    \"\"\"Apply consistent feature engineering to training and holdout data.\"\"\"\n    d = data.copy()\n\n    # Parse date and extract temporal features\n    d['dteday'] = pd.to_datetime(d['dteday'], format='mixed')\n    d['day_of_week'] = d['dteday'].dt.dayofweek  # 0=Monday, 6=Sunday\n    d['month'] = d['dteday'].dt.month\n\n    # Cyclical encoding for hour (captures 23->0 continuity)\n    d['hr_sin'] = np.sin(2 * np.pi * d['hr'] / 24)\n    d['hr_cos'] = np.cos(2 * np.pi * d['hr'] / 24)\n\n    # Cyclical encoding for month\n    d['month_sin'] = np.sin(2 * np.pi * d['month'] / 12)\n    d['month_cos'] = np.cos(2 * np.pi * d['month'] / 12)\n\n    # Behavioral indicators\n    d['is_rush_hour'] = d['hr'].apply(lambda h: 1 if h in [7, 8, 9, 16, 17, 18, 19] else 0)\n    d['is_night'] = d['hr'].apply(lambda h: 1 if h in [22, 23, 0, 1, 2, 3, 4, 5] else 0)\n    d['is_weekend'] = (d['day_of_week'] >= 5).astype(int)\n\n    # Interaction features\n    d['temp_x_hum'] = d['temp_c'] * d['hum']\n    d['temp_x_wind'] = d['temp_c'] * d['windspeed']\n    d['temp_squared'] = d['temp_c'] ** 2\n\n    # One-hot encode categoricals\n    d = pd.get_dummies(d, columns=['season', 'weathersit', 'day_of_week', 'month'],\n                       prefix=['season', 'weather', 'dow', 'month'], drop_first=False)\n\n    # One-hot encode hour (captures non-linear hourly patterns)\n    d['hr_int'] = d['hr'].astype(int)\n    d = pd.get_dummies(d, columns=['hr_int'], prefix='hr', drop_first=False)\n\n    # Drop columns not needed for modeling\n    drop_cols = ['dteday', 'hr']\n    if is_training:\n        drop_cols += ['casual', 'registered']\n    d = d.drop(columns=[c for c in drop_cols if c in d.columns], errors='ignore')\n    return d\n\n# Apply to training data\ndf_eng = engineer_features(df, is_training=True)\ny = df_eng['total'].values\nX = df_eng.drop(columns=['total'])\n\nprint(f\"Engineered features: {X.shape[1]}\")\nprint(f\"Training samples: {X.shape[0]}\")\nprint(f\"\\nFeature categories:\")\nprint(f\"  Continuous: temp_c, feels_like_c, hum, windspeed + cyclical + interactions\")\nprint(f\"  One-hot: season(4), weather(3-4), dow(7), month(12), hour(24)\")\nprint(f\"  Binary: holiday, workingday, is_rush_hour, is_night, is_weekend\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing\n\nApply MinMaxScaler to continuous features as recommended by the course reading (Johnny's tip on feature scaling). Neural networks perform better when inputs are normalized to similar scales."
   ],
   "id": "cell-11"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Identify continuous features for scaling\ncontinuous_features = ['temp_c', 'feels_like_c', 'hum', 'windspeed',\n                       'hr_sin', 'hr_cos', 'month_sin', 'month_cos',\n                       'temp_x_hum', 'temp_x_wind', 'temp_squared']\n\n# Fit scaler on training data\nscaler = MinMaxScaler()\nX[continuous_features] = scaler.fit_transform(X[continuous_features])\n\n# Train/validation split (80/20)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training set:   {X_train.shape}\")\nprint(f\"Validation set: {X_val.shape}\")\nprint(f\"Target mean: {y_train.mean():.1f}, std: {y_train.std():.1f}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Network Model\n\nWe use scikit-learn's MLPRegressor \u2014 a multi-layer perceptron neural network for regression.\n- **Architecture**: 3 hidden layers (128, 64, 32 neurons) with ReLU activation\n- **Optimizer**: Adam with adaptive learning rate (initial lr=0.001)\n- **Regularization**: L2 penalty (alpha=0.0001) + early stopping\n- **Training**: Mini-batch (128 samples), up to 200 epochs with patience=15"
   ],
   "id": "cell-13"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build the MLPRegressor neural network\nmodel = MLPRegressor(\n    hidden_layer_sizes=(128, 64, 32),\n    activation='relu',\n    solver='adam',\n    learning_rate='adaptive',\n    learning_rate_init=0.001,\n    alpha=0.0001,  # L2 regularization\n    max_iter=200,\n    early_stopping=True,\n    validation_fraction=0.1,\n    n_iter_no_change=15,\n    batch_size=128,\n    random_state=42,\n    verbose=True\n)\n\nprint(\"Training neural network...\")\nmodel.fit(X_train, y_train)\nprint(f\"\\nConverged in {model.n_iter_} iterations\")\nprint(f\"Best validation score: {model.best_validation_score_:.4f}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training History"
   ],
   "id": "cell-15"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot training loss curve\nplt.figure(figsize=(10, 5))\nplt.plot(model.loss_curve_, label='Training Loss', color='steelblue')\nplt.xlabel('Iteration')\nplt.ylabel('MSE Loss')\nplt.title('Training Loss Over Iterations')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-16"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Validation set performance\ny_val_pred = np.maximum(model.predict(X_val), 0)  # No negative predictions\n\nval_rmse = root_mean_squared_error(y_val, y_val_pred)\nval_r2 = r2_score(y_val, y_val_pred)\nval_mae = mean_absolute_error(y_val, y_val_pred)\n\nprint(\"=\" * 50)\nprint(\"     VALIDATION SET PERFORMANCE\")\nprint(\"=\" * 50)\nprint(f\"  RMSE:  {val_rmse:.2f}\")\nprint(f\"  R\\u00b2:    {val_r2:.4f}\")\nprint(f\"  MAE:   {val_mae:.2f}\")\nprint(\"=\" * 50)\n\n# Predicted vs Actual scatter\nplt.figure(figsize=(8, 8))\nplt.scatter(y_val, y_val_pred, alpha=0.1, s=5, color='steelblue')\nmax_val = max(y_val.max(), y_val_pred.max())\nplt.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Perfect prediction')\nplt.xlabel('Actual Total Rentals')\nplt.ylabel('Predicted Total Rentals')\nplt.title(f'Predicted vs Actual \\u2014 Validation Set\\nRMSE={val_rmse:.2f}, R\\u00b2={val_r2:.4f}')\nplt.legend()\nplt.tight_layout()\nplt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Mini Holdout Evaluation\n\nTest our model against the mini holdout set (with known answers) to estimate grading performance.\nThe grading notebook evaluates: RMSE, R\\u00b2, MAE, Median Absolute Error, and percent of predictions within 5%, 10%, 20% of actual."
   ],
   "id": "cell-18"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load mini holdout features and answers\nmini_features = pd.read_csv(MINI_URL)\nmini_answers = pd.read_csv(MINI_ANSWERS_URL)\nmini_actual = (mini_answers['casual'] + mini_answers['registered']).values\nprint(f\"Mini holdout samples: {len(mini_actual)}\")\n\n# Apply same feature engineering pipeline\nmini_eng = engineer_features(mini_features, is_training=False)\n\n# Ensure same columns as training (fill missing with 0)\nfor col in X.columns:\n    if col not in mini_eng.columns:\n        mini_eng[col] = 0\nmini_eng = mini_eng[X.columns]\n\n# Scale with the SAME fitted scaler\nmini_eng[continuous_features] = scaler.transform(mini_eng[continuous_features])\n\n# Predict\nmini_pred = np.maximum(model.predict(mini_eng), 0)\n\n# ===== KEY METRICS =====\nmini_rmse = root_mean_squared_error(mini_actual, mini_pred)\nmini_r2 = r2_score(mini_actual, mini_pred)\nmini_mae = mean_absolute_error(mini_actual, mini_pred)\n\nprint()\nprint(\"=\" * 55)\nprint(\"     MINI HOLDOUT PERFORMANCE (vs known answers)\")\nprint(\"=\" * 55)\nprint(f\"  RMSE:              {mini_rmse:.2f}\")\nprint(f\"  R\\u00b2:                {mini_r2:.4f}\")\nprint(f\"  MAE:               {mini_mae:.2f}\")\nprint(\"-\" * 55)\n\nfor pct in [5, 10, 20]:\n    within = np.mean(np.abs(mini_pred - mini_actual) <= mini_actual * (pct/100)) * 100\n    print(f\"  Within {pct}%:         {within:.1f}%\")\n\nprint(\"=\" * 55)\n\n# Save mini holdout predictions\nmini_pred_int = np.round(mini_pred).astype(int)\nmini_predictions_df = pd.DataFrame({'predictions': mini_pred_int})\nmini_predictions_df.to_csv('team8-module4-mini-predictions.csv', index=False)\nprint(f\"\\nSaved mini holdout predictions to team8-module4-mini-predictions.csv\")\nprint(f\"Shape: {mini_predictions_df.shape}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-19"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Mini holdout visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Scatter: predicted vs actual with 20% threshold coloring\npct_diff = np.abs(mini_pred - mini_actual) / np.maximum(mini_actual, 1)\ncolors = ['steelblue' if p <= 0.2 else 'coral' for p in pct_diff]\naxes[0].scatter(mini_actual, mini_pred, c=colors, alpha=0.5, s=20)\nmax_val = max(mini_actual.max(), mini_pred.max())\naxes[0].plot([0, max_val], [0, max_val], 'r--', linewidth=2)\naxes[0].set_xlabel('Actual Total Rentals')\naxes[0].set_ylabel('Predicted Total Rentals')\naxes[0].set_title(f'Mini Holdout: Predicted vs Actual\\nRMSE={mini_rmse:.2f}, R\\u00b2={mini_r2:.4f}')\n\n# Time series: actual vs predicted\naxes[1].plot(mini_actual, label='Actual', alpha=0.8, color='steelblue')\naxes[1].plot(mini_pred, label='Predicted', alpha=0.8, color='coral')\naxes[1].set_xlabel('Sample Index (hourly)')\naxes[1].set_ylabel('Total Rentals')\naxes[1].set_title('Mini Holdout: Actual vs Predicted Over Time')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Full Holdout Predictions (November/December)"
   ],
   "id": "cell-21"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load full holdout dataset\nholdout = pd.read_csv(HOLDOUT_URL)\nprint(f\"Holdout data shape: {holdout.shape}\")\n\n# Apply same feature engineering pipeline\nholdout_eng = engineer_features(holdout, is_training=False)\nfor col in X.columns:\n    if col not in holdout_eng.columns:\n        holdout_eng[col] = 0\nholdout_eng = holdout_eng[X.columns]\nholdout_eng[continuous_features] = scaler.transform(holdout_eng[continuous_features])\n\n# Predict\nholdout_pred = np.maximum(model.predict(holdout_eng), 0)\nholdout_pred = np.round(holdout_pred).astype(int)\n\nprint(f\"Predictions generated: {len(holdout_pred)}\")\nprint(f\"Prediction range: {holdout_pred.min()} - {holdout_pred.max()}\")\nprint(f\"Mean prediction: {holdout_pred.mean():.1f}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-22"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save predictions CSV for submission\npredictions_df = pd.DataFrame({'predictions': holdout_pred})\npredictions_df.to_csv('team8-module4-predictions.csv', index=False)\nprint(f\"Saved predictions to team8-module4-predictions.csv\")\nprint(f\"Shape: {predictions_df.shape}\")\nprint(f\"\\nFirst 10 predictions:\")\ndisplay(predictions_df.head(10))"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-23"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Analysis & Visualizations"
   ],
   "id": "cell-24"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Residual analysis on validation set\nresiduals = y_val - y_val_pred\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Residual distribution\naxes[0].hist(residuals, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\naxes[0].axvline(x=0, color='red', linestyle='--')\naxes[0].set_xlabel('Residual (Actual - Predicted)')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('Residual Distribution')\n\n# Residuals vs Predicted\naxes[1].scatter(y_val_pred, residuals, alpha=0.05, s=5, color='steelblue')\naxes[1].axhline(y=0, color='red', linestyle='--')\naxes[1].set_xlabel('Predicted Values')\naxes[1].set_ylabel('Residuals')\naxes[1].set_title('Residuals vs Predicted Values')\n\n# Hourly prediction patterns\ny_all_pred = np.maximum(model.predict(X), 0)\npred_df = pd.DataFrame({'actual': y, 'predicted': y_all_pred, 'hr': df['hr'].values})\nhourly_comp = pred_df.groupby('hr').agg({'actual': 'mean', 'predicted': 'mean'})\naxes[2].bar(hourly_comp.index - 0.2, hourly_comp['actual'], 0.4, label='Actual', color='steelblue')\naxes[2].bar(hourly_comp.index + 0.2, hourly_comp['predicted'], 0.4, label='Predicted', color='coral')\naxes[2].set_xlabel('Hour of Day')\naxes[2].set_ylabel('Average Total Rentals')\naxes[2].set_title('Actual vs Predicted by Hour')\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-25"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Permutation importance\nsample_idx = np.random.choice(len(X_val), min(3000, len(X_val)), replace=False)\nX_sample = X_val.iloc[sample_idx]\ny_sample = y_val[sample_idx]\n\nperm_imp = permutation_importance(model, X_sample, y_sample, n_repeats=5,\n                                   random_state=42, scoring='neg_mean_absolute_error')\n\n# Top 15 features\nimp_df = pd.DataFrame({\n    'feature': X.columns,\n    'importance': perm_imp.importances_mean\n}).sort_values('importance', ascending=True).tail(15)\n\nplt.figure(figsize=(10, 8))\nplt.barh(imp_df['feature'], imp_df['importance'], color='steelblue')\nplt.xlabel('Importance (decrease in MAE when permuted)')\nplt.title('Top 15 Feature Importances (Permutation)')\nplt.tight_layout()\nplt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "cell-26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Discussion Questions\n\n### Q1: Network Architecture (Zhao, CEO)\n**\"How many layers should the network have, and which hyperparameter has the most potential for improvement?\"**\n\nOur final model uses **3 hidden layers (128 \\u2192 64 \\u2192 32 neurons)** with ReLU activation. We found that **(B) learning rate and optimizer selection** had the most impact on model performance. The Adam optimizer with an initial learning rate of 0.001 and adaptive scheduling gave the best results. We also found that adding one-hot encoding for the hour feature (24 binary columns) was more impactful than adding more layers beyond 3.\n\n### Q2: Feature Engineering (Johnny, Intern)\n**\"How should we handle the temperature features?\"**\n\nWe kept both `temp_c` (actual temperature) and `feels_like_c` (feels-like temperature) and applied MinMaxScaler normalization as recommended by the course reading. We also created interaction features (`temp_c \\u00d7 humidity`, `temp_c \\u00d7 windspeed`) and a quadratic term (`temp_squared`) to capture the non-linear relationship between temperature and demand (rentals peak at moderate temperatures, not at extremes).\n\n### Q3: Learning Rate Optimization (Zhao, CEO)\n**\"What approach will you take to find the optimal learning rate?\"**\n\nWe used scikit-learn's adaptive learning rate approach:\n1. Start with Adam optimizer's default learning rate (0.001)\n2. The `adaptive` learning rate policy automatically reduces the rate when training loss plateaus\n3. Early stopping with patience of 15 epochs prevents overfitting\n4. This avoids exhaustive grid search while finding good convergence\n\n### Q4: Loss Function (Johnny, Intern)\n**\"What loss function will you use? How will we know if the model has strong predictive power?\"**\n\nWe use **Mean Squared Error (MSE)** as the loss function because it penalizes large prediction errors more heavily, which is important for demand forecasting where big misses are costly.\n\nWe evaluate with multiple metrics:\n- **RMSE** (root of MSE): interpretable in same units as target\n- **R-squared**: proportion of variance explained (1.0 = perfect)\n- **MAE**: average absolute error in bike count\n\n### Q5: Ethics \\u2014 Insurance Pricing (William, Investment Banker)\n**\"Should we predict damage likelihood based on user profile data for insurance pricing?\"**\n\nWe recommend **(D) Use real-time GPS behavioral monitoring** instead of profile-based data. Using name, birthday, sex, and address for damage prediction risks:\n- **Discriminatory pricing** based on demographics (similar to redlining)\n- **Privacy violations** even with consent (consent can be coerced)\n- **Legal liability** under fair lending and anti-discrimination laws\n\nInstead, GPS-based behavioral monitoring (with transparent user consent) assesses actual riding behavior, not demographic proxies.\n\n### Q6: Post-Pandemic Recovery (Zhao, CEO)\n**\"When should bikes be pulled for cleaning? Are there lasting pandemic effects?\"**\n\nBased on weather and usage patterns in the data:\n- **Cleaning schedule**: Rotate bikes during **low-demand hours** (midnight\\u20135 AM) and during **severe weather events** (weathersit = 3 or 4) when demand drops significantly\n- **Pandemic effects**: Casual ridership has recovered, but commuter patterns (registered users) may have permanently shifted due to remote work. Our model shows that `is_weekend` and `is_rush_hour` are important features, and changes in commuter behavior would affect rush hour demand\n- **Recommendation**: Monitor the ratio of casual-to-registered riders over time to detect structural changes"
   ],
   "id": "cell-27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Results\n\n### Model Performance\n\n| Metric | Validation | Mini Holdout |\n|--------|-----------|--------------|\n| **RMSE** | ~152 | **~224** |\n| **R-squared** | ~0.804 | **~0.622** |\n| **MAE** | ~95 | **~164** |\n\n### Key Model Features\n- 3-layer MLP neural network (128, 64, 32) with ReLU activation\n- 67 engineered features including one-hot hour, cyclical encodings, and interaction terms\n- MinMaxScaler preprocessing on continuous features\n- Adam optimizer with adaptive learning rate and early stopping\n\n### Business Recommendations for WelcomeBike DC\n1. **Staffing**: Deploy extra bikes during rush hours (7-9 AM, 4-7 PM) on workdays\n2. **Weather response**: Reduce inventory during rain/snow events (weathersit 3-4)\n3. **Seasonal planning**: Summer and fall have highest demand; winter requires reduced fleet\n4. **Location strategy**: Focus stations near transit hubs and popular commute routes\n5. **Revenue optimization**: Dynamic pricing during peak demand hours could increase revenue"
   ],
   "id": "cell-28"
  }
 ]
}